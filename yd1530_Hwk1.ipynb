## For Assignment 1

### https://www.kaggle.com/uciml/pima-indians-diabetes-database - Download the indian diabetes dataset

1.Import relevant commands for numpy, pandas, sklearn.

import numpy as np
from pandas import DataFrame, read_csv, to_numeric
from sklearn import cluster
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier


2.Using the appropriate pandas function, read the diabetes.csv into a dataframe. Pay good attention to the necessary arguments.

data = read_csv('diabetes.csv', sep=",")
# Calling DataFrame constructor 
df = DataFrame(data)
print(df)

3.use naivebayes, logistic regression and 3-nn classifiers (library) to train on the training sets and 
compute training and validation errors for each fold (see the diabetes_10fold_train_val.zip file, XT01...XT10: training sets, XV01....XV10: corresponding validation sets). The target label is Outcome.

#Create a Gaussian Classifier
gnb = GaussianNB()

#Create a logistic regression classifier
lrclassifier = LogisticRegression(random_state=0, max_iter=1000)

#Create a knn classifier
knn = KNeighborsClassifier(n_neighbors=3)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

#find the mean at the end
mean_nb = 0

# create a list to store the file numbers
file_number = ['01','02','03','04','05','06','07','08','09','10']

# loop here to calculate the error for each training set
for i in file_number:
    
    trainingSet = '/media/anuja/study/Anuja/MS/CS_697A(ML)/hw1/diabetes_10fold_train_val/XT'+i+'.csv'
    XT = read_csv(trainingSet, sep=",")
    validationSet = '/media/anuja/study/Anuja/MS/CS_697A(ML)/hw1/diabetes_10fold_train_val/XV'+i+'.csv'
    VT = read_csv(validationSet, sep=",")
    
    # all features in x train
    X_train =  XT.iloc[:,1:8]
    # outcome or label in Y train
    Y_train = XT.iloc[:,8]

    #Train the all 3 models using the training sets
    gnb.fit(X_train, Y_train)
    logistic = lrclassifier.fit(X_train, Y_train)
    fit   = knn.fit(X_train, Y_train)

    #predict the response for training data itself
    XT_features =  XT.iloc[:,1:8]
    y_pred_nb_train = gnb.predict(XT_features)
    y_pred_nn_train = fit.predict(XT_features)
    y_pred_lr_train = logistic.predict(XT_features)
    XT_label_train =  XT.iloc[:,8]
    accuracy_nb_train = metrics.accuracy_score(XT_label_train, y_pred_nb_train)
    accuracy_lr_train = metrics.accuracy_score(XT_label_train, y_pred_lr_train)
    accuracy_nn_train = metrics.accuracy_score(XT_label_train, y_pred_nn_train)
    
    
    #Predict the response for validation dataset
    VT_features =  VT.iloc[:,1:8]
    y_pred_nb = gnb.predict(VT_features)
    y_pred_nn = fit.predict(VT_features)
    y_pred_lr = logistic.predict(VT_features)
    VT_label =  VT.iloc[:,8]
    accuracy_nb = metrics.accuracy_score(VT_label, y_pred_nb)
    accuracy_lr = metrics.accuracy_score(VT_label, y_pred_lr)
    accuracy_nn = metrics.accuracy_score(VT_label, y_pred_nn)

    mean_nb = mean_nb + (1 - accuracy_nb)
    mean_lr = mean_nb + (1 - accuracy_lr)
    mean_nn = mean_nb + (1 - accuracy_nn)

    # Model Accuracy and Error
    print("\n Error for validation set and training set "+ i+ ": \n")
    print("NB: Validation Error: "  , 1 - accuracy_nb, "; Training Error: ", 1 - accuracy_nb_train)
    print("LR: Validation Error: "  , 1 - accuracy_lr, "; Training Error: ", 1 - accuracy_lr_train)
    print("NN: Validation Error: "  , 1 - accuracy_nn, "; Training Error: ", 1 - accuracy_nn_train)

# calculating average error from 10 sets
print("\nFinal NB error: ",mean_nb/10)
print("Final LR error: ",mean_lr/10)
print("Final NN error: ",mean_nn/10)



4.is the error of naive bayes <0.2 with confidence 0.9





5.have  naive bayes and knn the same error?



6.do the three classifiers have different errors?

